# -*- coding: utf-8 -*-
"""AlexTele2Data_cb_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cd_U7GscfQzOB6II-v_4ox7FX8JtEaDt
"""

!pip install catboost

#baseline recommend_syst_pro by Alex-Tele2-Data team
import numpy as np
import pandas as pd  #for dataframes
from sklearn.model_selection import train_test_split  #to divide data into train and validate
from sklearn.metrics import roc_auc_score  #for metric
from catboost import CatBoostClassifier
import pickle #for saving model
RS=42  #we will fix random state

#let s connect to google drive with data
from google.colab import drive
drive.mount ('/content/gdrive', force_remount = True)

!ls '../content/gdrive/My Drive/dataset_MADE_2020_taskB'

data_df=pd.read_csv('../content/gdrive/My Drive/dataset_MADE_2020_taskB/train.csv',header=0)
data_df.head()

test_df=pd.read_csv('../content/gdrive/My Drive/dataset_MADE_2020_taskB/test.csv',
                      header=0)
test_df.head()

data_df.describe()

data_df['item_id'].nunique()



#divide dataset to train and test
df_x, x_val, df_y, y_val=train_test_split(data_df[['user_id','item_id','timestamp']],\
                                                data_df['like'], test_size=0.05,random_state=RS)

#pl_clf = RandomForestClassifier(n_estimators=200,max_depth=300,
#                                                            n_jobs=-1, \
#                                                  verbose=True,random_state=RS)
#pl_clf = LGBMClassifier(n_estimators=200,max_depth=200,
#                        n_jobs=-1,
#                        silent=False,
#                        random_state=RS)
pl_clf = CatBoostClassifier(iterations=1500,
                        task_type="GPU",
                        depth=8,
                        learning_rate=0.1,
                        random_seed=RS)

pl_clf.fit(df_x,df_y,eval_set=(x_val,y_val));

print('Скор на трейне',pl_clf.score(df_x, df_y))
print('Скор по валидации',pl_clf.score(x_val, y_val))

#let s save our model to use on server
filename = 'model_recommend_cb01.pickle'
pickle.dump(pl_clf, open(filename, 'wb'))

# This is example of code how to load saved model
# load the model from disk
# loaded_model = pickle.load(open(filename, 'rb'))
# prediction = loaded_model.predict(X_test)
# print(prediction)

#let s save model from colaboratory oto our PC
from google.colab import files
files.download('model_recommend_cb01.pickle')

#take most popular 300 items
top_items=data_df['item_id'].value_counts(ascending=False)[:300]
print(top_items)

print(top_items[:20])

lst_top20=[100,143,194,390,391,76,403,402,405,404,178,118,154,165,9,129,22,35,92,44]

data_df['like'].value_counts()

lst_out=[]  #list for resul list
N_items=20  #number of predicted items for user
for index, row in test_df.iterrows():
  temp_user=row[0]
  temp_time=row[1]
  cnt=0
  temp_lst=[]
  for i in top_items:
    pred=pl_clf.predict([temp_user,i,temp_time])
    print(temp_user,i,temp_time,pred)
    if pred:
      cnt+=1
      temp_lst.append(i)
      if cnt > N_items:
        break
  temp_set=set(temp_lst)
  temp_lst2=list(temp_set)  #избавляемся от дублей, откуда они взялись непонятно
  if len(temp_lst2) < 20:
    temp_lst2.extend(lst_top20[:20-len(temp_lst2)])
  lst_out.append(temp_lst2)

lst_out=[]  #list for resul list
N_items=20  #number of predicted items for user
for index, row in test_df.iterrows():
  temp_user=row[0]
  temp_time=row[1]
  cnt=0
  temp_lst=[]
  for i in top_items:
    pred=pl_clf.predict([temp_user,i,temp_time])
    print(temp_user,i,temp_time,pred)
    if pred:
      cnt+=1
      temp_lst.append(i)
      if cnt > N_items:
        break
  temp_set=set(temp_lst)
  temp_lst2=list(temp_set)  
  if len(temp_lst) < 20:
    temp_lst.extend(lst_top20[:20-len(temp_lst)])
  lst_out.append(temp_lst)

print(lst_out[:5])

lst_0=[]   #list for results
lst_1=[]
lst_2=[]
lst_3=[]
lst_4=[]
lst_5=[]
lst_6=[]
lst_7=[]
lst_8=[]
lst_9=[]
lst_10=[]
lst_11=[]
lst_12=[]
lst_13=[]
lst_14=[]
lst_15=[]
lst_16=[]
lst_17=[]
lst_18=[]
lst_19=[]
for j in lst_out:
  lst_0.append(j[0])
  lst_1.append(j[1])
  lst_2.append(j[2])
  lst_3.append(j[3])
  lst_4.append(j[4])
  lst_5.append(j[5])
  lst_6.append(j[6])
  lst_7.append(j[7])
  lst_8.append(j[8])
  lst_9.append(j[9])
  lst_10.append(j[10])
  lst_11.append(j[11])
  lst_12.append(j[12])
  lst_13.append(j[13])
  lst_14.append(j[14])
  lst_15.append(j[15])
  lst_16.append(j[16])
  lst_17.append(j[17])
  lst_18.append(j[18])
  lst_19.append(j[19])

test_df['0']=lst_0
test_df['1']=lst_1
test_df['2']=lst_2
test_df['3']=lst_3
test_df['4']=lst_4
test_df['5']=lst_5
test_df['6']=lst_6
test_df['7']=lst_7
test_df['8']=lst_8
test_df['9']=lst_9
test_df['10']=lst_10
test_df['11']=lst_11
test_df['12']=lst_12
test_df['13']=lst_13
test_df['14']=lst_14
test_df['15']=lst_15
test_df['16']=lst_16
test_df['17']=lst_17
test_df['18']=lst_18
test_df['19']=lst_19

test_df.head()

test_df[['user_id','0','1','2','3','4','5','6','7','8','9','10','11','12','13','14',\
        '15','16','17','18','19']].to_csv('submission01.csv',index=False,header=True)

!ls

from google.colab import files

files.download('submission01.csv')









